{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### probability to be next\n",
    "import time, math, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\"\"\"functions\"\"\"\n",
    "\n",
    "def logis(A):\n",
    "    \"\"\"logistic function\"\"\"\n",
    "    return 1/(1+np.exp(-A))\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"softmax fuction\"\"\"\n",
    "    return np.exp(Z)/np.sum(np.exp(Z))\n",
    "\n",
    "def cross_entropy(X, Y):\n",
    "    \"\"\"cross entropyãƒ¼\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    return -np.sum(Y*np.log(X))/n_samples  # devide by number of samples\n",
    "\n",
    "def grad_loss(X, Y, Z):\n",
    "    \"\"\"weight gradient of loss \"\"\"\n",
    "    n_samples = X.shape[0]        \n",
    "    dW = -np.dot(X.T, Y - Z)/n_samples\n",
    "    db = -np.dot(np.ones([1, n_samples]), Y - Z)/n_samples\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"input data\"\"\"\n",
    "data = pd.read_csv('/Users/sotainoue1/Desktop/Study/departure/probability/test_data.csv',sep=\",\")\n",
    "data\n",
    "X =np.zeros(shape=(48,2))\n",
    "for i in range(6):\n",
    "     for k in range(8):\n",
    "        X[i*8+k] = [data.values[k,2],data.values[k,3+i]]\n",
    "\n",
    "\"\"\"traning data set for test\"\"\"\n",
    "Y = np.array([[0],[1],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[0]])\n",
    "\n",
    "\"\"\"data for scikitlearn\"\"\"\n",
    "uma = Bunch(data=X,target=Y)\n",
    "\n",
    "X = uma.data\n",
    "Y = uma.target\n",
    "\n",
    "n_samples,n_features = X.shape\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "\n",
    "W  = np.ones([n_features,6]) ##initial weight\n",
    "b = 0.1 ##initial bias\n",
    "loss = np.array([])\n",
    "\n",
    "for s in range(10):\n",
    "    # save loss and accuracy of training & test\n",
    "    loss = np.append(loss, cross_entropy(X, Y))\n",
    "    \n",
    "    for i in range(6):  ##Compute for each event \n",
    "        X_try = X[i*6 : i*6+8,0:2]\n",
    "        Y_try = Y[i*6:i*6+8]\n",
    "        \n",
    "        X_try_soc = preprocessing.minmax_scale(X_try[0:8,0]) ## Normalization \n",
    "        X_try_dis = preprocessing.minmax_scale(X_try[0:8,1])\n",
    "\n",
    "        z1 = np.dot(X_try_soc,W[0,i]) #social\n",
    "        z2 = np.dot(X_try_dis,W[1,i]) #distance\n",
    "\n",
    "        z3 = softmax(logis(z1+z2)+b)\n",
    "\n",
    "       \n",
    "        z4 = cross_entropy(z3,Y_try)\n",
    "\n",
    "        z5,z6 = grad_loss(X_try,Y_try, z3)\n",
    "\n",
    "        W[0,i] -= mu*np.sum(z5)\n",
    "        W[1,i] -= mu*np.sum(z5)  ####  ????????\n",
    "        b -= mu*np.sum(z6)\n",
    "        print(W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
